seed: 42
debug: false
output_dir: outputs
checkpoint_dir: /scratch/izar/cizinsky/garment-completion/models/checkpoints
max_train_samples: 1792000 # 28000*64
launch_cmd: null # will be set during runtime

logger:
  project: pbr-generation
  tags: [baseline]
  run_id: null # tqtzd06l

data:
  val_size: 300
  trn_debug_size: -1
  val_debug_size: -1
  val_sel_texture_names: [
    "Denim_Magenta_Floral_Print",
    "Herringbone_Lime_Green_Animal_Print_(Leopard)",
    "Georgette_Emerald_Green_Checks",
    "Lace_Pumpkin_Orange_Brocade",
    "Jacquard_Magenta_Animal_Print_(Leopard)"
  ]
  pbr_maps_path: "/scratch/izar/cizinsky/garment-completion/datasets/pbr_maps/dresscode"
  mask_path: "/scratch/izar/cizinsky/garment-completion/datasets/masks/shirt.png"
  num_workers: 20
  batch_size: 20 # baseline: 64
  res: 512 # baseline: 512

# Optimisation
optim:
  lr: 1e-4
  weight_decay: 1e-2
  warmup_steps: 1000

  # Scheduler
  # - reduce on plateau (default)
  plateau_patience: 7 # how many val/f1 to wait before reducing
  plateau_factor: 0.5 # reduce by this factor
  min_lr: 1e-6  # minimum learning rate
  plateau_mode: "min" # mode for plateau scheduler
  plateau_metric: "val/lpips" # metric to monitor for plateau scheduler
  # - cosine
  use_cosine_scheduler: true # baseline: false

  # Gradient clipping
  max_grad_norm: 1.0 # gradient clipping
  grad_norm_type: 2.0 # norm type for gradient clipping

  # Loss
  ddim_loss: true # baseline: true
  ddim_loss_weight: 0.75

  # EMA
  ema_decay: 0.999
  use_ema: true # baseline: true

model:
  vae_path: "/home/cizinsky/garment-texture-completion/data_generation/dresscode/material_gen"
  diffusion_path: "stable-diffusion-v1-5/stable-diffusion-v1-5" # stable-diffusion-v1-5/stable-diffusion-inpainting
  conditioning_dropout_prob: 0.05 # classifier-free guidance / baseline: 0.05

  # UNet
  train_from_scratch: false
  train_with_lora: false
  use_pretrained_unet: false # only for evaluation


  # type of unet
  is_inpainting: false # if yes, then the model expects one extra channel in the input (the mask)

trainer:
  max_steps: -1 #  will be computed during runtime as max_train_samples // batch_size / baseline: 28000 (with batch size 8)
  accelerator: gpu
  devices: 1
  checkpoints_total_limit: 2
  checkpoint_every_n_train_steps: 5000 # baseline: 4000
  val_check_interval: 1000 # in steps / baseline: one per epoch
  precision: 16-mixed
  log_every_n_steps: 10 # baseline: 1


hydra:
  run:
    dir: outputs/hydra/${now:%Y-%m-%d}_${now:%H-%M-%S}
  job:
    chdir: False