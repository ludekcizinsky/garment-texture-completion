{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42671bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4294968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "sys.path.append(os.path.abspath(\"..\"))  # Add the project root\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43fbdee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab424fa",
   "metadata": {},
   "source": [
    "### Dataloading\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff9df93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.dataset import InpaintingDataset\n",
    "from helpers import data_utils\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3600adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"../configs/train.yaml\")\n",
    "dataset = InpaintingDataset(cfg)\n",
    "i = random.randint(0, len(dataset) - 1)\n",
    "sample = dataset[i]\n",
    "\n",
    "diffuse_img, partial_img = sample[\"full_diffuse_img\"], sample[\"partial_diffuse_img\"]\n",
    "\n",
    "diffuse_img, partial_img = data_utils.channels_last(diffuse_img), data_utils.channels_last(partial_img)\n",
    "diffuse_img, partial_img = data_utils.denormalise_image(diffuse_img), data_utils.denormalise_image(partial_img)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].imshow(diffuse_img)\n",
    "ax[0].set_title(\"Full Diffuse Image\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].imshow(partial_img)\n",
    "ax[1].set_title(\"Partial Diffuse Image\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0069dd3",
   "metadata": {},
   "source": [
    "### Vae encoding and decoding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b635fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from helpers.dataset import InpaintingDataset\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from helpers import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59878fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_config = \"/home/cizinsky/garment-texture-completion/configs/train.yaml\"\n",
    "with open(path_to_config, \"r\") as f:\n",
    "    cfg = OmegaConf.load(f)\n",
    "\n",
    "vae_path = os.path.join(cfg.model.vae_path, \"refine_vae\")\n",
    "vae_diffuse = AutoencoderKL.from_pretrained(\n",
    "    vae_path,\n",
    "    subfolder=\"vae_checkpoint_diffuse\",\n",
    "    revision=\"fp32\",\n",
    "    local_files_only=True,\n",
    "    torch_dtype=torch.float32,\n",
    ").to(\"cuda\").eval()\n",
    "\n",
    "dataset = InpaintingDataset(cfg)\n",
    "\n",
    "\n",
    "sample = dataset[0]\n",
    "diffuse_img, partial_img = sample[\"full_diffuse_img\"][None], sample[\"partial_diffuse_img\"][None]\n",
    "diffuse_img.shape, partial_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b79a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_diffuse_img, tmp_partial_img = data_utils.channels_last(diffuse_img), data_utils.channels_last(partial_img)\n",
    "tmp_diffuse_img, tmp_partial_img = data_utils.denormalise_image(tmp_diffuse_img), data_utils.denormalise_image(tmp_partial_img)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].imshow(tmp_diffuse_img.squeeze(0))\n",
    "ax[0].set_title(\"Full Diffuse Image\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].imshow(tmp_partial_img.squeeze(0))\n",
    "ax[1].set_title(\"Partial Diffuse Image\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2f56fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode_image(image, vae):\n",
    "    # Encode\n",
    "    with torch.no_grad():\n",
    "        latents = vae.encode(image.cuda()).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor  # scale to match diffusion training\n",
    "\n",
    "    # Decode\n",
    "    with torch.no_grad():\n",
    "        decoded = vae.decode(latents / vae.config.scaling_factor).sample\n",
    "\n",
    "    return decoded\n",
    "\n",
    "def post_process_image(image):\n",
    "    decoded_image = data_utils.channels_last(data_utils.denormalise_image(image.cpu().numpy()))\n",
    "    return decoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c3412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_full, tensor_partial = torch.from_numpy(diffuse_img), torch.from_numpy(partial_img)\n",
    "decoded_full, decoded_partial = encode_decode_image(tensor_full, vae_diffuse), encode_decode_image(tensor_partial, vae_diffuse)\n",
    "decoded_full, decoded_partial = post_process_image(decoded_full), post_process_image(decoded_partial)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].imshow(decoded_full[0])\n",
    "ax[0].set_title(\"Decoded Full Diffuse Image\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].imshow(decoded_partial[0])\n",
    "ax[1].set_title(\"Decoded Partial Diffuse Image\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973bc88",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4ba1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.dataset import InpaintingDataset\n",
    "from helpers.pl_module import GarmentInpainterModule\n",
    "from helpers.data_utils import post_process_image\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04954714",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"../configs/train.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5546d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GarmentInpainterModule(cfg, None, None).to(\"cuda\").to(torch.float16).eval()\n",
    "model.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d74aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = InpaintingDataset(cfg)\n",
    "i = random.randint(0, len(dataset) - 1)\n",
    "sample = dataset[i]\n",
    "\n",
    "diffuse_img, partial_img = sample[\"full_diffuse_img\"], sample[\"partial_diffuse_img\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = partial_img\n",
    "post_img = post_process_image(image)\n",
    "\n",
    "plt.imshow(post_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c52c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = torch.from_numpy(image).unsqueeze(0)\n",
    "strengths = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "post_inpainted_imgs = []\n",
    "for strength in strengths:\n",
    "    inpainted_img = model.inference(x_in.cuda(), strength=strength, num_inference_steps=100)\n",
    "    post_inpainted_imgs.append(post_process_image(inpainted_img.to(torch.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, len(strengths), figsize=(20, 5))\n",
    "for i, post_inpainted_img in enumerate(post_inpainted_imgs):\n",
    "    ax[i].imshow(post_inpainted_img[0])\n",
    "    ax[i].axis(\"off\")\n",
    "    ax[i].set_title(f\"Strength: {strengths[i]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451041b9",
   "metadata": {},
   "source": [
    "### Metrics Computation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9944db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.dataset import InpaintingDataset\n",
    "from helpers.pl_module import GarmentInpainterModule\n",
    "from helpers.data_utils import post_process_image, denormalise_image\n",
    "from helpers.metrics import compute_ssim, compute_psnr, compute_lpips, compute_all_metrics\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ac0816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"../configs/train.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db91107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = InpaintingDataset(cfg)\n",
    "i = random.randint(0, len(dataset) - 1)\n",
    "sample = dataset[i]\n",
    "\n",
    "diffuse_img, partial_img = sample[\"full_diffuse_img\"], sample[\"partial_diffuse_img\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ea35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = diffuse_img\n",
    "post_img = post_process_image(image)\n",
    "\n",
    "plt.imshow(post_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efcfe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B x C x H x W\n",
    "diffuse_img.shape, diffuse_img.min(), diffuse_img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffuse_img_denormalised = torch.from_numpy(denormalise_image(diffuse_img)).unsqueeze(0)\n",
    "diffuse_img_denormalised.shape, diffuse_img_denormalised.min(), diffuse_img_denormalised.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpips = compute_lpips(diffuse_img_denormalised, diffuse_img_denormalised)\n",
    "ssim = compute_ssim(diffuse_img_denormalised, diffuse_img_denormalised)\n",
    "psnr = compute_psnr(diffuse_img_denormalised, diffuse_img_denormalised)\n",
    "\n",
    "print(f\"LPIPS: {lpips}, SSIM: {ssim}, PSNR: {psnr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d59975",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_noise = torch.randn(diffuse_img_denormalised.shape)\n",
    "lpips = compute_lpips(diffuse_img_denormalised + tiny_noise, diffuse_img_denormalised)\n",
    "ssim = compute_ssim(diffuse_img_denormalised + tiny_noise, diffuse_img_denormalised)\n",
    "psnr = compute_psnr(diffuse_img_denormalised + tiny_noise, diffuse_img_denormalised)\n",
    "\n",
    "print(f\"LPIPS: {lpips}, SSIM: {ssim}, PSNR: {psnr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d5efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "post_img = post_process_image(diffuse_img)\n",
    "ax[0].imshow(post_img)\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "tiny_noise_arr = tiny_noise.cpu().numpy()[0]\n",
    "post_img_with_noise = post_process_image(diffuse_img + tiny_noise_arr)\n",
    "ax[1].imshow(post_img_with_noise)\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].set_title(\"Image with Tiny Noise\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de6e6851",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GarmentInpainterModule(cfg, None, None).to(\"cuda\").to(torch.float16).eval()\n",
    "model.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430958dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = torch.from_numpy(partial_img).unsqueeze(0)\n",
    "denorm_full_img = torch.from_numpy(denormalise_image(diffuse_img)).unsqueeze(0)\n",
    "strengths = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "post_inpainted_imgs = []\n",
    "metrics = []\n",
    "for strength in strengths:\n",
    "    inpainted_img = model.inference(x_in.cuda(), strength=strength, num_inference_steps=100)\n",
    "    post_inpainted_imgs.append(post_process_image(inpainted_img.to(torch.float32)))\n",
    "\n",
    "    denorm_inpainted_img = torch.from_numpy(denormalise_image(inpainted_img.to(torch.float32).cpu().numpy()))\n",
    "    metrics.append(compute_all_metrics(denorm_inpainted_img, denorm_full_img))\n",
    "\n",
    "fig, ax = plt.subplots(1, len(strengths), figsize=(20, 5))\n",
    "for i, post_inpainted_img in enumerate(post_inpainted_imgs):\n",
    "    metrics_dict = metrics[i]\n",
    "    ax[i].imshow(post_inpainted_img[0])\n",
    "    ax[i].axis(\"off\")\n",
    "    ax[i].set_title(f\"Strength: {strengths[i]}\")\n",
    "    ax[i].text(0.5, -0.1, f\"SSIM: {metrics_dict['ssim']:.4f}\\nPSNR: {metrics_dict['psnr']:.4f}\\nLPIPS: {metrics_dict['lpips']:.4f}\",\n",
    "               ha='center', va='top', transform=ax[i].transAxes, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481a6aa",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9043a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.dataset import InpaintingDataset, get_dataloaders\n",
    "from helpers.pl_module import GarmentInpainterModule\n",
    "from helpers.data_utils import post_process_image, denormalise_image\n",
    "from helpers.metrics import compute_ssim, compute_psnr, compute_lpips, compute_all_metrics\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eac0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"../configs/train.yaml\")\n",
    "\n",
    "model = GarmentInpainterModule(cfg, None).to(\"cuda\").to(torch.float16).eval()\n",
    "model.setup()\n",
    "\n",
    "dataset = InpaintingDataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0299a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "_, val_loader = get_dataloaders(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d0085",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for batch in tqdm(val_loader, total=len(val_loader)):\n",
    "    batch_images = post_process_image(batch[\"full_diffuse_img\"])\n",
    "    batch_names = [path.split(\"/\")[-1] for path in batch[\"path\"]]\n",
    "    combined = []\n",
    "    for image, name in zip(batch_images, batch_names):\n",
    "        combined.append((image, name))\n",
    "    \n",
    "    images.extend(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_matplotlib(index):\n",
    "    img = images[index][0]\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    name = images[index][1]\n",
    "    plt.title(f\"Image {name}\")\n",
    "    plt.show()\n",
    "\n",
    "widgets.interact(show_image_matplotlib, index=widgets.IntSlider(0, 0, len(images) - 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c84411",
   "metadata": {},
   "source": [
    "### Testing different inference pipelines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a0ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_ROOT = '/scratch/izar/cizinsky/garment-completion/checkpoints'\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "import torch\n",
    "\n",
    "from diffusers import StableDiffusionInstructPix2PixPipeline, AutoencoderKL\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "from helpers.pl_module import GarmentInpainterModule\n",
    "from helpers.dataset import get_dataloaders\n",
    "from helpers.data_utils import denormalise_image_torch\n",
    "from helpers.data_utils import torch_image_to_pil, denormalise_image_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "156de079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKPT exists:  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cizinsky/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 10, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded!\n"
     ]
    }
   ],
   "source": [
    "run_name = \"lilac-hill-102\"\n",
    "checkpoint_path = f\"{CKPT_ROOT}/{run_name}/last.ckpt\"\n",
    "print(\"CKPT exists: \", os.path.exists(checkpoint_path))\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=False)\n",
    "cfg = checkpoint[\"hyper_parameters\"]\n",
    "trn_dataloader, val_dataloader = get_dataloaders(cfg)\n",
    "\n",
    "model = GarmentInpainterModule(cfg, trn_dataloader)\n",
    "model.setup()\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval().cuda()\n",
    "print(\"✅ Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a35df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained modules\n",
    "pretrained_model = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "\n",
    "vae_dir = \"/home/cizinsky/garment-texture-completion/data_generation/dresscode/material_gen\"\n",
    "vae_path = os.path.join(vae_dir, \"refine_vae\")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    vae_path,\n",
    "    subfolder=\"vae_checkpoint_diffuse\",\n",
    "    revision=\"fp32\",\n",
    "    local_files_only=True,\n",
    "    torch_dtype=torch.float32,\n",
    ").to(\"cuda\").eval()\n",
    "\n",
    "# Fine-tuned modules\n",
    "unet = model.model.unet\n",
    "\n",
    "# Scheduler\n",
    "scheduler = DDIMScheduler.from_pretrained(\n",
    "    pretrained_model,\n",
    "    subfolder=\"scheduler\",\n",
    "    revision=None,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aba38999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e85568406624ea7aed89998a3c9763e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_instruct_pix2pix.StableDiffusionInstructPix2PixPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "invpipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "        pretrained_model,\n",
    "        unet=unet,\n",
    "        vae=vae,\n",
    "        scheduler=scheduler,\n",
    "        revision=None,\n",
    "        safety_checker=None,\n",
    "        torch_dtype=torch.float32,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6063c5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cizinsky/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 10, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d982f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f2d7f3ae9247a6968ea0ea21a8e358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts = [\"fill the missing parts of a fabric texture matching the existing colors and style\"]*20\n",
    "zero_one_img_tensors = denormalise_image_torch(batch[\"partial_diffuse_img\"])\n",
    "preds = invpipe(\n",
    "    prompts,\n",
    "    image=zero_one_img_tensors,\n",
    "    num_inference_steps=50,\n",
    "    image_guidance_scale=1.5,\n",
    "    guidance_scale=7,\n",
    ").images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83cc2f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_images = [torch_image_to_pil(img) for img in denormalise_image_torch(batch[\"partial_diffuse_img\"])]\n",
    "target_images = [torch_image_to_pil(img) for img in denormalise_image_torch(batch[\"full_diffuse_img\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c231d195",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Image' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/cizinsky/garment-texture-completion/notebooks/unit_testing.ipynb Cell 47\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdev-node/home/cizinsky/garment-texture-completion/notebooks/unit_testing.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     axs[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39maxis(\u001b[39m\"\u001b[39m\u001b[39moff\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdev-node/home/cizinsky/garment-texture-completion/notebooks/unit_testing.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdev-node/home/cizinsky/garment-texture-completion/notebooks/unit_testing.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m interact(plot_images, index\u001b[39m=\u001b[39mIntSlider(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39;49m(preds)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, step\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m));\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Image' has no len()"
     ]
    }
   ],
   "source": [
    "def plot_images(index):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
    "    axs[0].imshow(cond_images[index])\n",
    "    axs[0].set_title(\"Condition\")\n",
    "    axs[0].axis(\"off\")\n",
    "    axs[1].imshow(preds[index])\n",
    "    axs[1].set_title(\"Predicted\")\n",
    "    axs[1].axis(\"off\")\n",
    "    axs[2].imshow(target_images[index])\n",
    "    axs[2].set_title(\"Target\")\n",
    "    axs[2].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_images, index=IntSlider(min=0, max=len(preds)-1, step=1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71099231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "garment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
