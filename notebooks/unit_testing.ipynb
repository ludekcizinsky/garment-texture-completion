{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42671bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4294968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "sys.path.append(os.path.abspath(\"..\"))  # Add the project root\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab424fa",
   "metadata": {},
   "source": [
    "### Dataloading\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff9df93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.dataset import InpaintingDataset\n",
    "from helpers import data_utils\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3600adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"../configs/train.yaml\")\n",
    "dataset = InpaintingDataset(cfg)\n",
    "i = random.randint(0, len(dataset) - 1)\n",
    "sample = dataset[i]\n",
    "\n",
    "diffuse_img, partial_img = sample[\"full_diffuse_img\"], sample[\"partial_diffuse_img\"]\n",
    "\n",
    "diffuse_img, partial_img = data_utils.channels_last(diffuse_img), data_utils.channels_last(partial_img)\n",
    "diffuse_img, partial_img = data_utils.denormalise_image(diffuse_img), data_utils.denormalise_image(partial_img)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].imshow(diffuse_img)\n",
    "ax[0].set_title(\"Full Diffuse Image\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].imshow(partial_img)\n",
    "ax[1].set_title(\"Partial Diffuse Image\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0069dd3",
   "metadata": {},
   "source": [
    "### Vae encoding and decoding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b635fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from helpers.dataset import InpaintingDataset\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from helpers import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59878fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_config = \"/home/cizinsky/garment-texture-completion/configs/train.yaml\"\n",
    "with open(path_to_config, \"r\") as f:\n",
    "    cfg = OmegaConf.load(f)\n",
    "\n",
    "vae_path = os.path.join(cfg.model.vae_path, \"refine_vae\")\n",
    "vae_diffuse = AutoencoderKL.from_pretrained(\n",
    "    vae_path,\n",
    "    subfolder=\"vae_checkpoint_diffuse\",\n",
    "    revision=\"fp32\",\n",
    "    local_files_only=True,\n",
    "    torch_dtype=torch.float32,\n",
    ").to(\"cuda\").eval()\n",
    "\n",
    "dataset = InpaintingDataset(cfg)\n",
    "\n",
    "\n",
    "sample = dataset[0]\n",
    "diffuse_img, partial_img = sample[\"full_diffuse_img\"][None], sample[\"partial_diffuse_img\"][None]\n",
    "diffuse_img.shape, partial_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b79a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_diffuse_img, tmp_partial_img = data_utils.channels_last(diffuse_img), data_utils.channels_last(partial_img)\n",
    "tmp_diffuse_img, tmp_partial_img = data_utils.denormalise_image(tmp_diffuse_img), data_utils.denormalise_image(tmp_partial_img)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].imshow(tmp_diffuse_img.squeeze(0))\n",
    "ax[0].set_title(\"Full Diffuse Image\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].imshow(tmp_partial_img.squeeze(0))\n",
    "ax[1].set_title(\"Partial Diffuse Image\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2f56fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode_image(image, vae):\n",
    "    # Encode\n",
    "    with torch.no_grad():\n",
    "        latents = vae.encode(image.cuda()).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor  # scale to match diffusion training\n",
    "\n",
    "    # Decode\n",
    "    with torch.no_grad():\n",
    "        decoded = vae.decode(latents / vae.config.scaling_factor).sample\n",
    "\n",
    "    return decoded\n",
    "\n",
    "def post_process_image(image):\n",
    "    decoded_image = data_utils.channels_last(data_utils.denormalise_image(image.cpu().numpy()))\n",
    "    return decoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c3412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_full, tensor_partial = torch.from_numpy(diffuse_img), torch.from_numpy(partial_img)\n",
    "decoded_full, decoded_partial = encode_decode_image(tensor_full, vae_diffuse), encode_decode_image(tensor_partial, vae_diffuse)\n",
    "decoded_full, decoded_partial = post_process_image(decoded_full), post_process_image(decoded_partial)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].imshow(decoded_full[0])\n",
    "ax[0].set_title(\"Decoded Full Diffuse Image\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].imshow(decoded_partial[0])\n",
    "ax[1].set_title(\"Decoded Partial Diffuse Image\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973bc88",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4ba1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.dataset import InpaintingDataset\n",
    "from helpers.pl_module import GarmentInpainterModule\n",
    "from helpers.data_utils import post_process_image\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04954714",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"../configs/train.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5546d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GarmentInpainterModule(cfg, None, None).to(\"cuda\").to(torch.float16).eval()\n",
    "model.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d74aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = InpaintingDataset(cfg)\n",
    "i = random.randint(0, len(dataset) - 1)\n",
    "sample = dataset[i]\n",
    "\n",
    "diffuse_img, partial_img = sample[\"full_diffuse_img\"], sample[\"partial_diffuse_img\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = partial_img\n",
    "post_img = post_process_image(image)\n",
    "\n",
    "plt.imshow(post_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c52c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = torch.from_numpy(image).unsqueeze(0)\n",
    "strengths = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "post_inpainted_imgs = []\n",
    "for strength in strengths:\n",
    "    inpainted_img = model.inference(x_in.cuda(), strength=strength, num_inference_steps=100)\n",
    "    post_inpainted_imgs.append(post_process_image(inpainted_img.to(torch.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, len(strengths), figsize=(20, 5))\n",
    "for i, post_inpainted_img in enumerate(post_inpainted_imgs):\n",
    "    ax[i].imshow(post_inpainted_img[0])\n",
    "    ax[i].axis(\"off\")\n",
    "    ax[i].set_title(f\"Strength: {strengths[i]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451041b9",
   "metadata": {},
   "source": [
    "### Metrics Computation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9944db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.dataset import InpaintingDataset\n",
    "from helpers.pl_module import GarmentInpainterModule\n",
    "from helpers.data_utils import post_process_image, denormalise_image\n",
    "from helpers.metrics import compute_ssim, compute_psnr, compute_lpips, compute_all_metrics\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac0816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"../configs/train.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db91107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = InpaintingDataset(cfg)\n",
    "i = random.randint(0, len(dataset) - 1)\n",
    "sample = dataset[i]\n",
    "\n",
    "diffuse_img, partial_img = sample[\"full_diffuse_img\"], sample[\"partial_diffuse_img\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ea35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = diffuse_img\n",
    "post_img = post_process_image(image)\n",
    "\n",
    "plt.imshow(post_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efcfe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B x C x H x W\n",
    "diffuse_img.shape, diffuse_img.min(), diffuse_img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffuse_img_denormalised = torch.from_numpy(denormalise_image(diffuse_img)).unsqueeze(0)\n",
    "diffuse_img_denormalised.shape, diffuse_img_denormalised.min(), diffuse_img_denormalised.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpips = compute_lpips(diffuse_img_denormalised, diffuse_img_denormalised)\n",
    "ssim = compute_ssim(diffuse_img_denormalised, diffuse_img_denormalised)\n",
    "psnr = compute_psnr(diffuse_img_denormalised, diffuse_img_denormalised)\n",
    "\n",
    "print(f\"LPIPS: {lpips}, SSIM: {ssim}, PSNR: {psnr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d59975",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_noise = torch.randn(diffuse_img_denormalised.shape)\n",
    "lpips = compute_lpips(diffuse_img_denormalised + tiny_noise, diffuse_img_denormalised)\n",
    "ssim = compute_ssim(diffuse_img_denormalised + tiny_noise, diffuse_img_denormalised)\n",
    "psnr = compute_psnr(diffuse_img_denormalised + tiny_noise, diffuse_img_denormalised)\n",
    "\n",
    "print(f\"LPIPS: {lpips}, SSIM: {ssim}, PSNR: {psnr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d5efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "post_img = post_process_image(diffuse_img)\n",
    "ax[0].imshow(post_img)\n",
    "ax[0].axis(\"off\")\n",
    "ax[0].set_title(\"Original Image\")\n",
    "\n",
    "tiny_noise_arr = tiny_noise.cpu().numpy()[0]\n",
    "post_img_with_noise = post_process_image(diffuse_img + tiny_noise_arr)\n",
    "ax[1].imshow(post_img_with_noise)\n",
    "ax[1].axis(\"off\")\n",
    "ax[1].set_title(\"Image with Tiny Noise\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de6e6851",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GarmentInpainterModule(cfg, None, None).to(\"cuda\").to(torch.float16).eval()\n",
    "model.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430958dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = torch.from_numpy(partial_img).unsqueeze(0)\n",
    "denorm_full_img = torch.from_numpy(denormalise_image(diffuse_img)).unsqueeze(0)\n",
    "strengths = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "post_inpainted_imgs = []\n",
    "metrics = []\n",
    "for strength in strengths:\n",
    "    inpainted_img = model.inference(x_in.cuda(), strength=strength, num_inference_steps=100)\n",
    "    post_inpainted_imgs.append(post_process_image(inpainted_img.to(torch.float32)))\n",
    "\n",
    "    denorm_inpainted_img = torch.from_numpy(denormalise_image(inpainted_img.to(torch.float32).cpu().numpy()))\n",
    "    metrics.append(compute_all_metrics(denorm_inpainted_img, denorm_full_img))\n",
    "\n",
    "fig, ax = plt.subplots(1, len(strengths), figsize=(20, 5))\n",
    "for i, post_inpainted_img in enumerate(post_inpainted_imgs):\n",
    "    metrics_dict = metrics[i]\n",
    "    ax[i].imshow(post_inpainted_img[0])\n",
    "    ax[i].axis(\"off\")\n",
    "    ax[i].set_title(f\"Strength: {strengths[i]}\")\n",
    "    ax[i].text(0.5, -0.1, f\"SSIM: {metrics_dict['ssim']:.4f}\\nPSNR: {metrics_dict['psnr']:.4f}\\nLPIPS: {metrics_dict['lpips']:.4f}\",\n",
    "               ha='center', va='top', transform=ax[i].transAxes, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481a6aa",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9043a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cizinsky/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cizinsky/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/cizinsky/venvs/garment/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "from helpers.dataset import InpaintingDataset, get_dataloaders\n",
    "from helpers.pl_module import GarmentInpainterModule\n",
    "from helpers.data_utils import post_process_image, denormalise_image\n",
    "from helpers.metrics import compute_ssim, compute_psnr, compute_lpips, compute_all_metrics\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eac0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"../configs/train.yaml\")\n",
    "\n",
    "model = GarmentInpainterModule(cfg, None).to(\"cuda\").to(torch.float16).eval()\n",
    "model.setup()\n",
    "\n",
    "dataset = InpaintingDataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0299a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "_, val_loader = get_dataloaders(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a5d0085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  9.73it/s]\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "for batch in tqdm(val_loader, total=len(val_loader)):\n",
    "    batch_images = post_process_image(batch[\"full_diffuse_img\"])\n",
    "    batch_names = [path.split(\"/\")[-1] for path in batch[\"path\"]]\n",
    "    combined = []\n",
    "    for image, name in zip(batch_images, batch_names):\n",
    "        combined.append((image, name))\n",
    "    \n",
    "    images.extend(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a81231d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb148b8847f34eb8a767ce39547ea0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=299), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_image_matplotlib(index):\n",
    "    img = images[index][0]\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    name = images[index][1]\n",
    "    plt.title(f\"Image {name}\")\n",
    "    plt.show()\n",
    "\n",
    "widgets.interact(show_image_matplotlib, index=widgets.IntSlider(0, 0, len(images) - 1));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "garment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
